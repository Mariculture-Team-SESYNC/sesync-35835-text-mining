---
title: "Cleaning ProQuest data and simple LDA topic model example"
author: "Quentin D. Read"
date: "3/8/2021"
output: html_document
---

**This is a work in progress and will be edited! The data cleaning part is in good shape but the modeling example is a "quick and dirty" solution that has flaws and could be improved.**

# Version History

- **3 March 2021**: first version
- **8 March 2021**: add information on how to tokenize into phrases of length `n` instead of words, thresholding out very common and very rare words, and using topic coherence score to help determine the optimal number of topics `k`.

# Summary

This is a notebook which goes through the process of cleaning the data from ProQuest, putting it in a format that can be analyzed, and fitting a very basic LDA model.

This borrows heavily from [Julia Silge's tidy textmining lesson on topic models linked here](https://juliasilge.github.io/tidytext/articles/topic_modeling.html) as well as [SESYNC's lesson on textmining linked here](https://cyberhelp.sesync.org/text-mining-lesson). 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Setup

Load packages (including all packages needed for the topic modeling).

```{r load, message = FALSE, warning = FALSE}
library(tidyverse)
library(tidytext)
library(tm)
library(topicmodels)
library(topicdoc)
```

# Parse raw output from ProQuest

Next, Andres wrote an awesome Python function to parse the raw output from ProQuest, including the full text and all the other fields (publisher, date, location, etc.). Luckily you can run Python code from within R, including from within R notebooks like this one, thanks to the `reticulate` package which is running behind the scenes here. This next bit of code is Python, which imports the Python module Andres wrote, and then parses the text document from ProQuest and save as a CSV in the same directory (which is inside your team's research data directory).

**Note** that the `text_parser.py` script we are importing has to be in the same directory as this notebook. If you are interested in the Python code, take a look at that script on the GitHub repo.

```{python}
import text_parser
text_parser.main(path = '/nfs/mariculture-data/Text_Parser/data/',
                 input = 'ProQuestDocuments.txt',
                 output = 'ProQuestDocuments_parsed')
```

Now we can load the resulting CSV into R.

```{r, message = FALSE}
articles <- read_csv('/nfs/mariculture-data/Text_Parser/data/ProQuestDocuments_parsed.csv')
```

# Put data into analyzable form

You can see that there is a lot of rich metadata about each article in ProQuest, along with the full text of each article:

```{r}
head(articles)
```

For the purposes of this example, we will just ignore any of that metadata. We'll just work with the numeric ID of each article and the full text. Select only those columns and rename them to get rid of spaces in the column names.

```{r}
articles <- articles %>%
  select(`ProQuest document ID`, `Full text`) %>%
  setNames(c('document_ID', 'full_text'))
```

We are going to fit a very simple LDA topic model that does not account for any structure above the level of individual words. Later we can work on phrases. Again this is basically taken from Julia Silge's tutorial linked above.

Next separate the character string of each article's full text into individual words. 

```{r}
articles_word <- articles %>%
  unnest_tokens(word, full_text)
```

**Note**: the `unnest_tokens()` function is powerful. By default, it tokenizes down to the length of one word. However, you can tokenize in a number of different ways. If you are interested in phrases of length *n*, or so-called *n-grams*, you can tokenize, as in this example using phrases of length 2: `articles %>% unnest_tokens(ngram, full_text, token = 'ngrams', n = 2)`. Read the documentation at `?unnest_tokens` for more info. I didn't implement any of this in this example notebook, but I did want to call it to your attention in case you are interested in exploring that.

Back to data cleaning: next we use the built-in lexicon of stop words to remove grammatical words that don't contribute to meaning, and make a table of the number of times the remaining words appear in each article.

```{r}
articles_wordcount <- articles_word %>%
  group_by(document_ID) %>%
  anti_join(stop_words) %>%
  count(word, sort = TRUE)
```

Here's some more manual cleaning. First, words appearing in only one article will not help in grouping articles by topic, but they can add to computation time. So let's remove those. Also, let's remove any words that appear in over half the articles. Again, those won't help that much because they will appear in almost every topic. That will get rid of words like "aquaculture," "fish," and "water." First remove the word "aquaculture," as it appears in every article by necessity and therefore won't be useful for differentiating topics. I also noticed a few common junk words that I removed manually. Later you could add to that list. **Note**: you could experiment with the low and high thresholds.

```{r}
n_articles_by_word <- articles_wordcount %>%
  group_by(word) %>%
  summarize(n_articles = length(unique(document_ID)))

singleton_words <- n_articles_by_word %>% 
  filter(n_articles == 1) %>% 
  pull(word)

too_common_words <- n_articles_by_word %>% 
  filter(n_articles > length(unique(articles$document_ID))/2) %>%
  pull(word)

junk_words <- c('Ã¢', 'https', 'doi.org')

too_common_words

articles_wordcount <- articles_wordcount %>% 
  filter(!word %in% c(singleton_words, too_common_words, junk_words))
```

In addition, remove any "words" that are actually numbers (such as years) which appear often in the articles. To do this, remove any row that is not `NA` when we convert the character strings to numeric.

```{r, warning = FALSE}
articles_wordcount <- articles_wordcount %>% filter(is.na(as.numeric(word)))
```

Finally, convert the long data frame, which has one row for each article-word combination, to a document-term matrix (DTM). The DTM is in wide form, where each row is an article and each column is a word, with the entries being how many times the word appears in the article. It is a relatively "sparse" matrix, meaning that there are a lot of zeroes in it, around 94% of the entries (because the majority of words only appear in one or two articles). After our data cleaning and thresholding we are down to about 3000 unique words from an initial 8000.

```{r}
articles_dtm <- articles_wordcount %>%
  cast_dtm(document = document_ID, term = word, value = n)

articles_dtm
```

# Fit the LDA model

The LDA model requires that we specify a number of topics a priori. I used `k = 4` for now but different numbers may be used. (Note we also set a seed because it's a stochastic algorithm so we need to ensure we get the same result each time for reproducibility.) Then use the `tidy()` function to extract output from the model. The result is a bunch of coefficients called `beta` for each word for each topic, showing how strongly that word is related to that topic.

```{r, message = FALSE}
articles_lda4 <- LDA(articles_dtm, k = 4, control = list(seed = 410))

articles_lda4_output <- tidy(articles_lda4)
```

# Look at output from the model

Find the top 10 terms, with the highest beta values, for each topic.

```{r}
top10 <- articles_lda4_output %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup %>%
  arrange(topic, -beta)

print(top10, n = 40)
```

Create a plot to visualize the results.

```{r}
theme_set(theme_bw())

top10 %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta)) +
  geom_bar(stat = "identity") +
  scale_x_reordered() +
  scale_y_continuous(expand = expansion(mult = c(0, 0.03))) +
  facet_wrap(~ topic, scales = "free_x") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

In this toy example, it looks like there is a topic about how management activities in Alabama affect different wild species, one about federal management and policy (committee, NOAA, research), one about some kind of fish farming operation in Florida (pen, farming, Sarasota), and one very broad "big-picture" topic (production, commercial, sustainable, development).

# Associating the articles back to topics

Now that we have our (arbitrarily chosen 4) topics at the word level, we can go back and assign the articles to those topics. We might not be able to cleanly assign each article to a single topic because articles could contain words from multiple topics. So we will get a score for each article for each topic and we can tentatively assign the article to the topic it scores highest for.

The following returns the coefficient `gamma` for each topic for each document. This is essentially a score that shows how highly each document was associated with each topic.

```{r}
articles_lda_gamma <- tidy(articles_lda4, matrix = "gamma")
articles_lda_gamma
```

Let's look at how confidently each article was assigned to a topic. For each article, find the topic with the highest gamma and show what it was. A gamma approaching 1 means approaching 100% confidence. The histograms show that the majority of articles were assigned almost completely to one of the four topics, but a significant number were divided between two topics.

```{r}
articles_lda_gamma %>% 
  group_by(document) %>%
  filter(gamma == max(gamma)) %>%
  ggplot(aes(x = gamma)) +
    facet_wrap(~ topic) +
    geom_histogram()
```

Here is an alternative way to plot this result. First sort the gamma data frame by the top topic for each document to neaten the plot, then plot a stacked bar showing the proportion of gamma for each topic for each document.

```{r}
# Find order of documents for plotting
sorted_doc_order <- articles_lda_gamma %>%
  group_by(document) %>%
  mutate(consensus_topic = topic[gamma == max(gamma)], consensus_gamma = max(gamma)) %>%
  arrange(consensus_topic, -consensus_gamma) %>%
  pull(document) %>%
  unique

# Make plot
articles_lda_gamma %>% 
  mutate(document = factor(as.character(document), levels = sorted_doc_order)) %>%
ggplot(aes(x = document, y = gamma, fill = factor(topic))) +
  geom_bar(position = 'stack', stat = 'identity') +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5)) +
  scale_y_continuous(expand = c(0, 0)) +
  scale_fill_brewer(palette = "Dark2")
```

You could use this result to look at the articles that are supposed to be part of the same topic. In particular it looks like topic 2 is not very well defined as there are only a couple of articles that are confidently assigned to that topic.

# Determining topic coherence

In the example above I picked `k = 4` as the number of topics completely arbitrarily. You may have more informed reasons to pick a different number of topics. However we can use a measure of "fit" called topic coherence to help determine what the optimal number of topics might be. Coherence is a score for a particular topic which represents how well-defined the topic is. Therefore we would prefer a value of `k` where the mean coherence across all topics is as high as possible. Let's go ahead and fit LDA models with all values of `k` between 2 and 10 and see whether this helps us distinguish. We will use the top 10 words per topic to assess coherence (although you could choose other values).

```{r}
ks <- 2:10

# Get nine reproducible random seeds
set.seed(999)
random_seeds <- sample(1:1000, length(ks))

lda_list <- map2(ks, random_seeds, ~ LDA(articles_dtm, k = .x, control = list(seed = .y)))
mean_coherence <- map_dbl(lda_list, ~ mean(topic_coherence(., articles_dtm, top_n_tokens = 10)))

plot(ks, mean_coherence, type = 'b')
```

From this initial quick and dirty look, it seems that 2 and 4 are decently well-supported values of `k`. You could use this analysis to supplement your informed judgment about what the best values of `k` might be.

# Additional resources

I found these other resources while browsing around -- you might find them helpful.

- [Vignette on text mining with the textmineR package](https://cran.r-project.org/web/packages/textmineR/vignettes/c_topic_modeling.html). This seems to be a nice package that would be a good alternative to the tidyverse ones and might actually have better features.
- [Beginner's guide to LDA modeling](https://towardsdatascience.com/beginners-guide-to-lda-topic-modelling-with-r-e57a5a8e7a25). Uses the `textmineR` package.

